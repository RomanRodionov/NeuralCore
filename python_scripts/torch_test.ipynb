{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets\n",
    "from torchvision.transforms import ToTensor\n",
    "\n",
    "# Download training data from open datasets.\n",
    "training_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n",
    "\n",
    "# Download test data from open datasets.\n",
    "test_data = datasets.MNIST(\n",
    "    root=\"data\",\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=ToTensor(),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of X [N, C, H, W]: torch.Size([64, 1, 28, 28])\n",
      "Shape of y: torch.Size([64]) torch.int64\n",
      "Using cuda device\n",
      "NeuralNetwork(\n",
      "  (flatten): Flatten(start_dim=1, end_dim=-1)\n",
      "  (linear_relu_stack): Sequential(\n",
      "    (0): Linear(in_features=784, out_features=512, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=512, out_features=512, bias=True)\n",
      "    (3): ReLU()\n",
      "    (4): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "batch_size = 64\n",
    "\n",
    "# Create data loaders.\n",
    "train_dataloader = DataLoader(training_data, batch_size=batch_size)\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size)\n",
    "\n",
    "for X, y in test_dataloader:\n",
    "    print(f\"Shape of X [N, C, H, W]: {X.shape}\")\n",
    "    print(f\"Shape of y: {y.shape} {y.dtype}\")\n",
    "    break\n",
    "\n",
    "# Get cpu, gpu or mps device for training.\n",
    "device = (\n",
    "    \"cuda\"\n",
    "    if torch.cuda.is_available()\n",
    "    else \"mps\"\n",
    "    if torch.backends.mps.is_available()\n",
    "    else \"cpu\"\n",
    ")\n",
    "print(f\"Using {device} device\")\n",
    "\n",
    "# Define model\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(28*28, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork().to(device)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=1e-3)\n",
    "\n",
    "def train(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    model.train()\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        X, y = X.to(device), y.to(device)\n",
    "\n",
    "        # Compute prediction error\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if batch % 100 == 0:\n",
    "            loss, current = loss.item(), (batch + 1) * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def test(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    num_batches = len(dataloader)\n",
    "    model.eval()\n",
    "    test_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X, y = X.to(device), y.to(device)\n",
    "            pred = model(X)\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "    test_loss /= num_batches\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.645947  [   64/60000]\n",
      "loss: 1.567707  [ 6464/60000]\n",
      "loss: 1.651063  [12864/60000]\n",
      "loss: 1.453689  [19264/60000]\n",
      "loss: 1.487172  [25664/60000]\n",
      "loss: 1.469282  [32064/60000]\n",
      "loss: 1.404597  [38464/60000]\n",
      "loss: 1.512972  [44864/60000]\n",
      "loss: 1.381101  [51264/60000]\n",
      "loss: 1.336121  [57664/60000]\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 1.350729  [   64/60000]\n",
      "loss: 1.244236  [ 6464/60000]\n",
      "loss: 1.339114  [12864/60000]\n",
      "loss: 1.165264  [19264/60000]\n",
      "loss: 1.183462  [25664/60000]\n",
      "loss: 1.159586  [32064/60000]\n",
      "loss: 1.107911  [38464/60000]\n",
      "loss: 1.235351  [44864/60000]\n",
      "loss: 1.123422  [51264/60000]\n",
      "loss: 1.075255  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 79.9%, Avg loss: 1.043530 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 1.111046  [   64/60000]\n",
      "loss: 0.989692  [ 6464/60000]\n",
      "loss: 1.077824  [12864/60000]\n",
      "loss: 0.950332  [19264/60000]\n",
      "loss: 0.960872  [25664/60000]\n",
      "loss: 0.931401  [32064/60000]\n",
      "loss: 0.889410  [38464/60000]\n",
      "loss: 1.023115  [44864/60000]\n",
      "loss: 0.943674  [51264/60000]\n",
      "loss: 0.893621  [57664/60000]\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.942933  [   64/60000]\n",
      "loss: 0.818327  [ 6464/60000]\n",
      "loss: 0.891784  [12864/60000]\n",
      "loss: 0.804837  [19264/60000]\n",
      "loss: 0.811656  [25664/60000]\n",
      "loss: 0.778993  [32064/60000]\n",
      "loss: 0.740567  [38464/60000]\n",
      "loss: 0.874791  [44864/60000]\n",
      "loss: 0.822893  [51264/60000]\n",
      "loss: 0.775413  [57664/60000]\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.827039  [   64/60000]\n",
      "loss: 0.702998  [ 6464/60000]\n",
      "loss: 0.762885  [12864/60000]\n",
      "loss: 0.707349  [19264/60000]\n",
      "loss: 0.708591  [25664/60000]\n",
      "loss: 0.676501  [32064/60000]\n",
      "loss: 0.636184  [38464/60000]\n",
      "loss: 0.771623  [44864/60000]\n",
      "loss: 0.737890  [51264/60000]\n",
      "loss: 0.697010  [57664/60000]\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.744044  [   64/60000]\n",
      "loss: 0.620895  [ 6464/60000]\n",
      "loss: 0.671114  [12864/60000]\n",
      "loss: 0.639895  [19264/60000]\n",
      "loss: 0.633331  [25664/60000]\n",
      "loss: 0.605470  [32064/60000]\n",
      "loss: 0.559184  [38464/60000]\n",
      "loss: 0.698674  [44864/60000]\n",
      "loss: 0.674430  [51264/60000]\n",
      "loss: 0.643044  [57664/60000]\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.681860  [   64/60000]\n",
      "loss: 0.559394  [ 6464/60000]\n",
      "loss: 0.603043  [12864/60000]\n",
      "loss: 0.591352  [19264/60000]\n",
      "loss: 0.575277  [25664/60000]\n",
      "loss: 0.554383  [32064/60000]\n",
      "loss: 0.500371  [38464/60000]\n",
      "loss: 0.645909  [44864/60000]\n",
      "loss: 0.624638  [51264/60000]\n",
      "loss: 0.604482  [57664/60000]\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.633283  [   64/60000]\n",
      "loss: 0.511542  [ 6464/60000]\n",
      "loss: 0.550518  [12864/60000]\n",
      "loss: 0.555215  [19264/60000]\n",
      "loss: 0.528800  [25664/60000]\n",
      "loss: 0.516576  [32064/60000]\n",
      "loss: 0.454465  [38464/60000]\n",
      "loss: 0.606638  [44864/60000]\n",
      "loss: 0.584553  [51264/60000]\n",
      "loss: 0.576016  [57664/60000]\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.593983  [   64/60000]\n",
      "loss: 0.473500  [ 6464/60000]\n",
      "loss: 0.508686  [12864/60000]\n",
      "loss: 0.527392  [19264/60000]\n",
      "loss: 0.490738  [25664/60000]\n",
      "loss: 0.487827  [32064/60000]\n",
      "loss: 0.418031  [38464/60000]\n",
      "loss: 0.576623  [44864/60000]\n",
      "loss: 0.552035  [51264/60000]\n",
      "loss: 0.554263  [57664/60000]\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.561377  [   64/60000]\n",
      "loss: 0.442745  [ 6464/60000]\n",
      "loss: 0.474458  [12864/60000]\n",
      "loss: 0.505446  [19264/60000]\n",
      "loss: 0.458953  [25664/60000]\n",
      "loss: 0.465417  [32064/60000]\n",
      "loss: 0.388670  [38464/60000]\n",
      "loss: 0.552924  [44864/60000]\n",
      "loss: 0.525336  [51264/60000]\n",
      "loss: 0.537193  [57664/60000]\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.533724  [   64/60000]\n",
      "loss: 0.417551  [ 6464/60000]\n",
      "loss: 0.445738  [12864/60000]\n",
      "loss: 0.487741  [19264/60000]\n",
      "loss: 0.432140  [25664/60000]\n",
      "loss: 0.447614  [32064/60000]\n",
      "loss: 0.364671  [38464/60000]\n",
      "loss: 0.533712  [44864/60000]\n",
      "loss: 0.503236  [51264/60000]\n",
      "loss: 0.523468  [57664/60000]\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.509867  [   64/60000]\n",
      "loss: 0.396714  [ 6464/60000]\n",
      "loss: 0.421207  [12864/60000]\n",
      "loss: 0.473092  [19264/60000]\n",
      "loss: 0.409360  [25664/60000]\n",
      "loss: 0.433180  [32064/60000]\n",
      "loss: 0.344756  [38464/60000]\n",
      "loss: 0.517733  [44864/60000]\n",
      "loss: 0.484774  [51264/60000]\n",
      "loss: 0.512203  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 88.6%, Avg loss: 0.423952 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.489009  [   64/60000]\n",
      "loss: 0.379326  [ 6464/60000]\n",
      "loss: 0.399916  [12864/60000]\n",
      "loss: 0.460804  [19264/60000]\n",
      "loss: 0.389902  [25664/60000]\n",
      "loss: 0.421212  [32064/60000]\n",
      "loss: 0.327945  [38464/60000]\n",
      "loss: 0.504184  [44864/60000]\n",
      "loss: 0.469133  [51264/60000]\n",
      "loss: 0.502783  [57664/60000]\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.470542  [   64/60000]\n",
      "loss: 0.364655  [ 6464/60000]\n",
      "loss: 0.381184  [12864/60000]\n",
      "loss: 0.450296  [19264/60000]\n",
      "loss: 0.373109  [25664/60000]\n",
      "loss: 0.411060  [32064/60000]\n",
      "loss: 0.313589  [38464/60000]\n",
      "loss: 0.492471  [44864/60000]\n",
      "loss: 0.455758  [51264/60000]\n",
      "loss: 0.494727  [57664/60000]\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.453978  [   64/60000]\n",
      "loss: 0.352264  [ 6464/60000]\n",
      "loss: 0.364526  [12864/60000]\n",
      "loss: 0.441164  [19264/60000]\n",
      "loss: 0.358533  [25664/60000]\n",
      "loss: 0.402291  [32064/60000]\n",
      "loss: 0.301188  [38464/60000]\n",
      "loss: 0.482203  [44864/60000]\n",
      "loss: 0.444150  [51264/60000]\n",
      "loss: 0.487725  [57664/60000]\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.439030  [   64/60000]\n",
      "loss: 0.341719  [ 6464/60000]\n",
      "loss: 0.349579  [12864/60000]\n",
      "loss: 0.433110  [19264/60000]\n",
      "loss: 0.345784  [25664/60000]\n",
      "loss: 0.394590  [32064/60000]\n",
      "loss: 0.290343  [38464/60000]\n",
      "loss: 0.473034  [44864/60000]\n",
      "loss: 0.433950  [51264/60000]\n",
      "loss: 0.481601  [57664/60000]\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.425420  [   64/60000]\n",
      "loss: 0.332718  [ 6464/60000]\n",
      "loss: 0.336067  [12864/60000]\n",
      "loss: 0.425872  [19264/60000]\n",
      "loss: 0.334577  [25664/60000]\n",
      "loss: 0.387710  [32064/60000]\n",
      "loss: 0.280763  [38464/60000]\n",
      "loss: 0.464751  [44864/60000]\n",
      "loss: 0.424836  [51264/60000]\n",
      "loss: 0.476169  [57664/60000]\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.412926  [   64/60000]\n",
      "loss: 0.324990  [ 6464/60000]\n",
      "loss: 0.323766  [12864/60000]\n",
      "loss: 0.419322  [19264/60000]\n",
      "loss: 0.324636  [25664/60000]\n",
      "loss: 0.381484  [32064/60000]\n",
      "loss: 0.272262  [38464/60000]\n",
      "loss: 0.457238  [44864/60000]\n",
      "loss: 0.416652  [51264/60000]\n",
      "loss: 0.471303  [57664/60000]\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.401359  [   64/60000]\n",
      "loss: 0.318298  [ 6464/60000]\n",
      "loss: 0.312542  [12864/60000]\n",
      "loss: 0.413393  [19264/60000]\n",
      "loss: 0.315734  [25664/60000]\n",
      "loss: 0.375766  [32064/60000]\n",
      "loss: 0.264607  [38464/60000]\n",
      "loss: 0.450329  [44864/60000]\n",
      "loss: 0.409211  [51264/60000]\n",
      "loss: 0.466878  [57664/60000]\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.390590  [   64/60000]\n",
      "loss: 0.312505  [ 6464/60000]\n",
      "loss: 0.302234  [12864/60000]\n",
      "loss: 0.407945  [19264/60000]\n",
      "loss: 0.307727  [25664/60000]\n",
      "loss: 0.370483  [32064/60000]\n",
      "loss: 0.257694  [38464/60000]\n",
      "loss: 0.443937  [44864/60000]\n",
      "loss: 0.402299  [51264/60000]\n",
      "loss: 0.462780  [57664/60000]\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.380504  [   64/60000]\n",
      "loss: 0.307500  [ 6464/60000]\n",
      "loss: 0.292741  [12864/60000]\n",
      "loss: 0.402915  [19264/60000]\n",
      "loss: 0.300480  [25664/60000]\n",
      "loss: 0.365564  [32064/60000]\n",
      "loss: 0.251436  [38464/60000]\n",
      "loss: 0.437982  [44864/60000]\n",
      "loss: 0.395869  [51264/60000]\n",
      "loss: 0.458975  [57664/60000]\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.371020  [   64/60000]\n",
      "loss: 0.303139  [ 6464/60000]\n",
      "loss: 0.284016  [12864/60000]\n",
      "loss: 0.398248  [19264/60000]\n",
      "loss: 0.293905  [25664/60000]\n",
      "loss: 0.360958  [32064/60000]\n",
      "loss: 0.245750  [38464/60000]\n",
      "loss: 0.432454  [44864/60000]\n",
      "loss: 0.389861  [51264/60000]\n",
      "loss: 0.455447  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 90.3%, Avg loss: 0.339478 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.362059  [   64/60000]\n",
      "loss: 0.299347  [ 6464/60000]\n",
      "loss: 0.276002  [12864/60000]\n",
      "loss: 0.393858  [19264/60000]\n",
      "loss: 0.287901  [25664/60000]\n",
      "loss: 0.356638  [32064/60000]\n",
      "loss: 0.240595  [38464/60000]\n",
      "loss: 0.427265  [44864/60000]\n",
      "loss: 0.384154  [51264/60000]\n",
      "loss: 0.452145  [57664/60000]\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.353590  [   64/60000]\n",
      "loss: 0.295975  [ 6464/60000]\n",
      "loss: 0.268575  [12864/60000]\n",
      "loss: 0.389743  [19264/60000]\n",
      "loss: 0.282354  [25664/60000]\n",
      "loss: 0.352585  [32064/60000]\n",
      "loss: 0.235902  [38464/60000]\n",
      "loss: 0.422387  [44864/60000]\n",
      "loss: 0.378675  [51264/60000]\n",
      "loss: 0.448993  [57664/60000]\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.345550  [   64/60000]\n",
      "loss: 0.293025  [ 6464/60000]\n",
      "loss: 0.261662  [12864/60000]\n",
      "loss: 0.385868  [19264/60000]\n",
      "loss: 0.277174  [25664/60000]\n",
      "loss: 0.348781  [32064/60000]\n",
      "loss: 0.231568  [38464/60000]\n",
      "loss: 0.417791  [44864/60000]\n",
      "loss: 0.373418  [51264/60000]\n",
      "loss: 0.445971  [57664/60000]\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.337874  [   64/60000]\n",
      "loss: 0.290375  [ 6464/60000]\n",
      "loss: 0.255269  [12864/60000]\n",
      "loss: 0.382181  [19264/60000]\n",
      "loss: 0.272331  [25664/60000]\n",
      "loss: 0.345167  [32064/60000]\n",
      "loss: 0.227563  [38464/60000]\n",
      "loss: 0.413447  [44864/60000]\n",
      "loss: 0.368376  [51264/60000]\n",
      "loss: 0.443063  [57664/60000]\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.330583  [   64/60000]\n",
      "loss: 0.288013  [ 6464/60000]\n",
      "loss: 0.249313  [12864/60000]\n",
      "loss: 0.378671  [19264/60000]\n",
      "loss: 0.267854  [25664/60000]\n",
      "loss: 0.341738  [32064/60000]\n",
      "loss: 0.223845  [38464/60000]\n",
      "loss: 0.409303  [44864/60000]\n",
      "loss: 0.363504  [51264/60000]\n",
      "loss: 0.440274  [57664/60000]\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.323589  [   64/60000]\n",
      "loss: 0.285886  [ 6464/60000]\n",
      "loss: 0.243760  [12864/60000]\n",
      "loss: 0.375345  [19264/60000]\n",
      "loss: 0.263672  [25664/60000]\n",
      "loss: 0.338500  [32064/60000]\n",
      "loss: 0.220367  [38464/60000]\n",
      "loss: 0.405392  [44864/60000]\n",
      "loss: 0.358815  [51264/60000]\n",
      "loss: 0.437559  [57664/60000]\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.316883  [   64/60000]\n",
      "loss: 0.283960  [ 6464/60000]\n",
      "loss: 0.238587  [12864/60000]\n",
      "loss: 0.372161  [19264/60000]\n",
      "loss: 0.259741  [25664/60000]\n",
      "loss: 0.335411  [32064/60000]\n",
      "loss: 0.217134  [38464/60000]\n",
      "loss: 0.401706  [44864/60000]\n",
      "loss: 0.354318  [51264/60000]\n",
      "loss: 0.434900  [57664/60000]\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.310456  [   64/60000]\n",
      "loss: 0.282184  [ 6464/60000]\n",
      "loss: 0.233730  [12864/60000]\n",
      "loss: 0.369088  [19264/60000]\n",
      "loss: 0.256061  [25664/60000]\n",
      "loss: 0.332476  [32064/60000]\n",
      "loss: 0.214109  [38464/60000]\n",
      "loss: 0.398217  [44864/60000]\n",
      "loss: 0.349955  [51264/60000]\n",
      "loss: 0.432346  [57664/60000]\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.304327  [   64/60000]\n",
      "loss: 0.280546  [ 6464/60000]\n",
      "loss: 0.229185  [12864/60000]\n",
      "loss: 0.366102  [19264/60000]\n",
      "loss: 0.252570  [25664/60000]\n",
      "loss: 0.329724  [32064/60000]\n",
      "loss: 0.211271  [38464/60000]\n",
      "loss: 0.394872  [44864/60000]\n",
      "loss: 0.345750  [51264/60000]\n",
      "loss: 0.429869  [57664/60000]\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.298436  [   64/60000]\n",
      "loss: 0.279024  [ 6464/60000]\n",
      "loss: 0.224902  [12864/60000]\n",
      "loss: 0.363238  [19264/60000]\n",
      "loss: 0.249235  [25664/60000]\n",
      "loss: 0.327099  [32064/60000]\n",
      "loss: 0.208613  [38464/60000]\n",
      "loss: 0.391666  [44864/60000]\n",
      "loss: 0.341645  [51264/60000]\n",
      "loss: 0.427357  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.2%, Avg loss: 0.303594 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.292778  [   64/60000]\n",
      "loss: 0.277600  [ 6464/60000]\n",
      "loss: 0.220875  [12864/60000]\n",
      "loss: 0.360472  [19264/60000]\n",
      "loss: 0.246043  [25664/60000]\n",
      "loss: 0.324605  [32064/60000]\n",
      "loss: 0.206118  [38464/60000]\n",
      "loss: 0.388551  [44864/60000]\n",
      "loss: 0.337649  [51264/60000]\n",
      "loss: 0.424916  [57664/60000]\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.287353  [   64/60000]\n",
      "loss: 0.276278  [ 6464/60000]\n",
      "loss: 0.217081  [12864/60000]\n",
      "loss: 0.357770  [19264/60000]\n",
      "loss: 0.242924  [25664/60000]\n",
      "loss: 0.322217  [32064/60000]\n",
      "loss: 0.203810  [38464/60000]\n",
      "loss: 0.385552  [44864/60000]\n",
      "loss: 0.333779  [51264/60000]\n",
      "loss: 0.422486  [57664/60000]\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.282121  [   64/60000]\n",
      "loss: 0.275050  [ 6464/60000]\n",
      "loss: 0.213497  [12864/60000]\n",
      "loss: 0.355150  [19264/60000]\n",
      "loss: 0.239900  [25664/60000]\n",
      "loss: 0.319961  [32064/60000]\n",
      "loss: 0.201617  [38464/60000]\n",
      "loss: 0.382686  [44864/60000]\n",
      "loss: 0.329987  [51264/60000]\n",
      "loss: 0.420036  [57664/60000]\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.277104  [   64/60000]\n",
      "loss: 0.273898  [ 6464/60000]\n",
      "loss: 0.210103  [12864/60000]\n",
      "loss: 0.352623  [19264/60000]\n",
      "loss: 0.237031  [25664/60000]\n",
      "loss: 0.317798  [32064/60000]\n",
      "loss: 0.199516  [38464/60000]\n",
      "loss: 0.379974  [44864/60000]\n",
      "loss: 0.326264  [51264/60000]\n",
      "loss: 0.417624  [57664/60000]\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.272285  [   64/60000]\n",
      "loss: 0.272701  [ 6464/60000]\n",
      "loss: 0.206911  [12864/60000]\n",
      "loss: 0.350151  [19264/60000]\n",
      "loss: 0.234325  [25664/60000]\n",
      "loss: 0.315686  [32064/60000]\n",
      "loss: 0.197519  [38464/60000]\n",
      "loss: 0.377362  [44864/60000]\n",
      "loss: 0.322603  [51264/60000]\n",
      "loss: 0.415241  [57664/60000]\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.267643  [   64/60000]\n",
      "loss: 0.271532  [ 6464/60000]\n",
      "loss: 0.203922  [12864/60000]\n",
      "loss: 0.347768  [19264/60000]\n",
      "loss: 0.231688  [25664/60000]\n",
      "loss: 0.313641  [32064/60000]\n",
      "loss: 0.195553  [38464/60000]\n",
      "loss: 0.374809  [44864/60000]\n",
      "loss: 0.319026  [51264/60000]\n",
      "loss: 0.412845  [57664/60000]\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.263165  [   64/60000]\n",
      "loss: 0.270396  [ 6464/60000]\n",
      "loss: 0.201071  [12864/60000]\n",
      "loss: 0.345450  [19264/60000]\n",
      "loss: 0.229175  [25664/60000]\n",
      "loss: 0.311690  [32064/60000]\n",
      "loss: 0.193687  [38464/60000]\n",
      "loss: 0.372341  [44864/60000]\n",
      "loss: 0.315480  [51264/60000]\n",
      "loss: 0.410394  [57664/60000]\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.258817  [   64/60000]\n",
      "loss: 0.269297  [ 6464/60000]\n",
      "loss: 0.198351  [12864/60000]\n",
      "loss: 0.343174  [19264/60000]\n",
      "loss: 0.226765  [25664/60000]\n",
      "loss: 0.309808  [32064/60000]\n",
      "loss: 0.191872  [38464/60000]\n",
      "loss: 0.369997  [44864/60000]\n",
      "loss: 0.312050  [51264/60000]\n",
      "loss: 0.407948  [57664/60000]\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.254575  [   64/60000]\n",
      "loss: 0.268197  [ 6464/60000]\n",
      "loss: 0.195737  [12864/60000]\n",
      "loss: 0.340911  [19264/60000]\n",
      "loss: 0.224475  [25664/60000]\n",
      "loss: 0.307959  [32064/60000]\n",
      "loss: 0.190136  [38464/60000]\n",
      "loss: 0.367751  [44864/60000]\n",
      "loss: 0.308661  [51264/60000]\n",
      "loss: 0.405597  [57664/60000]\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.250467  [   64/60000]\n",
      "loss: 0.267076  [ 6464/60000]\n",
      "loss: 0.193258  [12864/60000]\n",
      "loss: 0.338662  [19264/60000]\n",
      "loss: 0.222252  [25664/60000]\n",
      "loss: 0.306140  [32064/60000]\n",
      "loss: 0.188475  [38464/60000]\n",
      "loss: 0.365544  [44864/60000]\n",
      "loss: 0.305304  [51264/60000]\n",
      "loss: 0.403238  [57664/60000]\n",
      "Test Error: \n",
      " Accuracy: 91.9%, Avg loss: 0.280296 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.246469  [   64/60000]\n",
      "loss: 0.265881  [ 6464/60000]\n",
      "loss: 0.190879  [12864/60000]\n",
      "loss: 0.336433  [19264/60000]\n",
      "loss: 0.220110  [25664/60000]\n",
      "loss: 0.304350  [32064/60000]\n",
      "loss: 0.186894  [38464/60000]\n",
      "loss: 0.363415  [44864/60000]\n",
      "loss: 0.301985  [51264/60000]\n",
      "loss: 0.400863  [57664/60000]\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.242560  [   64/60000]\n",
      "loss: 0.264698  [ 6464/60000]\n",
      "loss: 0.188616  [12864/60000]\n",
      "loss: 0.334180  [19264/60000]\n",
      "loss: 0.218048  [25664/60000]\n",
      "loss: 0.302635  [32064/60000]\n",
      "loss: 0.185368  [38464/60000]\n",
      "loss: 0.361339  [44864/60000]\n",
      "loss: 0.298680  [51264/60000]\n",
      "loss: 0.398456  [57664/60000]\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.238763  [   64/60000]\n",
      "loss: 0.263560  [ 6464/60000]\n",
      "loss: 0.186437  [12864/60000]\n",
      "loss: 0.332006  [19264/60000]\n",
      "loss: 0.216056  [25664/60000]\n",
      "loss: 0.300973  [32064/60000]\n",
      "loss: 0.183921  [38464/60000]\n",
      "loss: 0.359352  [44864/60000]\n",
      "loss: 0.295390  [51264/60000]\n",
      "loss: 0.396041  [57664/60000]\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.235113  [   64/60000]\n",
      "loss: 0.262371  [ 6464/60000]\n",
      "loss: 0.184336  [12864/60000]\n",
      "loss: 0.329889  [19264/60000]\n",
      "loss: 0.214133  [25664/60000]\n",
      "loss: 0.299333  [32064/60000]\n",
      "loss: 0.182499  [38464/60000]\n",
      "loss: 0.357454  [44864/60000]\n",
      "loss: 0.292154  [51264/60000]\n",
      "loss: 0.393655  [57664/60000]\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.231580  [   64/60000]\n",
      "loss: 0.261197  [ 6464/60000]\n",
      "loss: 0.182313  [12864/60000]\n",
      "loss: 0.327758  [19264/60000]\n",
      "loss: 0.212289  [25664/60000]\n",
      "loss: 0.297662  [32064/60000]\n",
      "loss: 0.181080  [38464/60000]\n",
      "loss: 0.355608  [44864/60000]\n",
      "loss: 0.288941  [51264/60000]\n",
      "loss: 0.391291  [57664/60000]\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.228148  [   64/60000]\n",
      "loss: 0.260037  [ 6464/60000]\n",
      "loss: 0.180375  [12864/60000]\n",
      "loss: 0.325616  [19264/60000]\n",
      "loss: 0.210471  [25664/60000]\n",
      "loss: 0.295994  [32064/60000]\n",
      "loss: 0.179684  [38464/60000]\n",
      "loss: 0.353835  [44864/60000]\n",
      "loss: 0.285779  [51264/60000]\n",
      "loss: 0.388941  [57664/60000]\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.224801  [   64/60000]\n",
      "loss: 0.258924  [ 6464/60000]\n",
      "loss: 0.178505  [12864/60000]\n",
      "loss: 0.323525  [19264/60000]\n",
      "loss: 0.208737  [25664/60000]\n",
      "loss: 0.294406  [32064/60000]\n",
      "loss: 0.178300  [38464/60000]\n",
      "loss: 0.352023  [44864/60000]\n",
      "loss: 0.282576  [51264/60000]\n",
      "loss: 0.386621  [57664/60000]\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.221529  [   64/60000]\n",
      "loss: 0.257798  [ 6464/60000]\n",
      "loss: 0.176712  [12864/60000]\n",
      "loss: 0.321457  [19264/60000]\n",
      "loss: 0.207028  [25664/60000]\n",
      "loss: 0.292886  [32064/60000]\n",
      "loss: 0.176937  [38464/60000]\n",
      "loss: 0.350241  [44864/60000]\n",
      "loss: 0.279409  [51264/60000]\n",
      "loss: 0.384286  [57664/60000]\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "epochs = 50\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train(train_dataloader, model, loss_fn, optimizer)\n",
    "    if t%10==1:\n",
    "        test(test_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "odict_keys(['linear_relu_stack.0.weight', 'linear_relu_stack.0.bias', 'linear_relu_stack.2.weight', 'linear_relu_stack.2.bias', 'linear_relu_stack.4.weight', 'linear_relu_stack.4.bias'])\n",
      "Saved PyTorch Model State to model.pth\n"
     ]
    }
   ],
   "source": [
    "torch.save(model.state_dict(), \"model.pth\")\n",
    "print(model.state_dict().keys())\n",
    "print(\"Saved PyTorch Model State to model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from json import JSONEncoder\n",
    "import json\n",
    "\n",
    "class EncodeTensor(JSONEncoder, torch.utils.data.Dataset):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, torch.Tensor):\n",
    "            return obj.cpu().detach().numpy().tolist()\n",
    "        return super(EncodeTensor, self).default(obj)\n",
    "\n",
    "with open('torch_weights.json', 'w') as json_file:\n",
    "    json.dump(model.state_dict(), json_file,cls=EncodeTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(512, 784) 2\n",
      "(512,) 1\n",
      "(512, 512) 2\n",
      "(512,) 1\n",
      "(10, 512) 2\n",
      "(10,) 1\n"
     ]
    }
   ],
   "source": [
    "with open('weights.bin', 'wb' ) as f: \n",
    "  for key, value in model.state_dict().items():\n",
    "    arr = value.cpu().detach().numpy()\n",
    "    print(arr.shape, arr.ndim)\n",
    "    f.write(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.0279671   0.00938362 -0.02349045 ... -0.01774568  0.03277795\n",
      "   0.03243467]\n",
      " [ 0.01100129  0.01008523  0.03203167 ... -0.01453802  0.03108073\n",
      "  -0.01675717]\n",
      " [-0.00873586 -0.01778367 -0.01929924 ...  0.00893023  0.00272433\n",
      "  -0.02481144]\n",
      " ...\n",
      " [-0.01636105  0.01383047 -0.0268059  ...  0.02908236  0.00608143\n",
      "   0.02359631]\n",
      " [ 0.01431939 -0.01392078 -0.0309662  ...  0.00408989  0.03498786\n",
      "  -0.01710635]\n",
      " [ 0.00312846  0.01463996  0.03497315 ... -0.01467601  0.01955105\n",
      "   0.01244057]]\n",
      "[ 2.90196319e-03  4.30605225e-02 -1.82794605e-03  3.34332250e-02\n",
      "  1.70415733e-02  4.75736195e-03  2.93841958e-02  1.67108979e-02\n",
      " -6.12115080e-04 -2.55043972e-02  3.96722443e-02  3.22613679e-02\n",
      "  7.06300559e-03  3.90617400e-02  1.86100474e-03  1.84419174e-02\n",
      "  3.38026113e-03 -8.95516574e-03 -3.60312080e-03  1.91219803e-02\n",
      " -2.32318565e-02 -1.19523648e-02  2.74080113e-02 -7.06782890e-03\n",
      "  3.11347060e-02  4.03756052e-02  1.49224130e-02  4.29224281e-04\n",
      " -9.92678106e-03  5.79616800e-03  7.27736857e-03  9.69306845e-03\n",
      "  7.80284731e-03  1.48812458e-02  6.96565583e-03  1.63905174e-02\n",
      " -1.08273197e-02  6.68255985e-03 -1.79852098e-02  1.66368820e-02\n",
      " -1.83460172e-02 -1.46885756e-02  1.42012779e-02  2.81354389e-03\n",
      "  5.34523763e-02 -1.17365774e-02  3.60280052e-02  1.75267626e-02\n",
      " -5.81169594e-03  9.31817945e-03  8.12527444e-03 -4.39572148e-03\n",
      "  6.88110944e-04  3.57643850e-02  8.10973998e-03 -1.18830400e-02\n",
      "  3.41159664e-02  2.92985290e-02  2.92875357e-02  2.68307049e-02\n",
      "  2.44475380e-02 -2.20971983e-02  5.46251759e-02  6.08896092e-03\n",
      " -2.49583367e-02  3.85306813e-02  3.59576866e-02  5.03618009e-02\n",
      "  6.32988894e-03  4.40739952e-02 -1.91254485e-02  3.89756262e-02\n",
      " -2.38043424e-02  5.99254528e-03  8.43456574e-03  4.35269922e-02\n",
      " -3.39761674e-02  5.76645844e-02 -9.65475384e-03  2.84409802e-02\n",
      "  9.49073874e-04  3.26801613e-02  1.88908484e-02  2.48387698e-02\n",
      "  1.91791244e-02  2.85113789e-02  5.58954440e-02  4.47845571e-02\n",
      "  1.75613519e-02 -3.79646127e-03 -2.39048004e-02  3.98469642e-02\n",
      " -3.37432930e-03 -2.14473568e-02  4.75715138e-02  4.45887521e-02\n",
      "  1.06497835e-04  4.91769165e-02 -1.74802821e-02  5.39652817e-02\n",
      "  2.18294635e-02 -2.67050080e-02 -2.12012716e-02 -8.67622066e-03\n",
      "  5.30257113e-02 -7.92740099e-03 -2.70529296e-02  1.71335898e-02\n",
      "  4.28731255e-02  9.39567853e-03  3.33870053e-02 -1.22201850e-03\n",
      " -2.79896217e-03  1.75090060e-02 -2.63080988e-02  4.69490848e-02\n",
      "  4.39885492e-03  1.68180577e-02 -4.29055467e-02 -2.05362923e-02\n",
      "  3.94406699e-04  5.05950376e-02  1.32880826e-02  3.08687775e-03\n",
      " -4.72913384e-02  6.13233587e-03 -1.67765524e-02 -3.70655116e-03\n",
      "  2.50421166e-02  2.38464703e-03  2.83926465e-02 -2.29214225e-02\n",
      " -7.21377460e-03  2.22251024e-02  7.53302639e-03 -2.24374738e-02\n",
      "  4.20274921e-02  4.21119332e-02  3.08869630e-02  1.56146754e-02\n",
      " -1.21514099e-02  1.21560588e-03  4.07986604e-02 -1.13044828e-02\n",
      "  2.19263206e-03 -3.44121158e-02 -1.55455321e-02 -3.79620492e-02\n",
      " -1.37229171e-02  3.69389472e-03  9.43030510e-03  1.36257338e-04\n",
      " -7.86028523e-03  5.51446192e-02  4.73550446e-02  2.04755273e-02\n",
      " -2.17921045e-02  3.24892625e-02  1.48542020e-02 -5.35755930e-03\n",
      "  1.41673479e-02  1.11561008e-02  3.99017781e-02  2.96833180e-02\n",
      "  1.55782430e-02  4.32289205e-02  2.05040872e-02 -3.13505940e-02\n",
      "  4.02356908e-02  3.25541198e-02  3.15998383e-02  4.15702239e-02\n",
      "  4.40912247e-02  1.96896736e-02 -1.27675598e-02 -1.34612815e-02\n",
      " -1.04356725e-02  5.58616221e-02 -3.95779572e-02 -3.43984999e-02\n",
      " -1.62334796e-02  3.65269408e-02 -1.02497302e-02 -2.97157206e-02\n",
      " -2.26035602e-02  2.26826780e-02  2.95439623e-02  8.41067359e-03\n",
      " -7.75278034e-03  1.18962480e-02 -5.54778962e-05  4.60304180e-03\n",
      "  2.22511813e-02  4.38663270e-03 -1.76629368e-02  9.95413214e-03\n",
      "  4.29859124e-02  2.34301072e-02  1.75006241e-02 -1.47500522e-02\n",
      " -1.83145683e-02  3.75267901e-02  4.07975391e-02  2.33844258e-02\n",
      "  7.37753734e-02  4.77316342e-02 -9.84190498e-03 -1.57145280e-02\n",
      "  2.60158489e-03  3.69625427e-02  1.08217439e-02  2.52504796e-02\n",
      "  3.66048925e-02  3.43491621e-02  4.96803932e-02 -3.31211067e-03\n",
      "  7.87063837e-02 -2.30714362e-02 -1.04544433e-02  6.67572767e-02\n",
      " -1.42888166e-02  2.82425527e-02  1.91420335e-02 -3.11786532e-02\n",
      "  2.42330674e-02  4.99652885e-02  1.68627556e-02  1.96029413e-02\n",
      " -1.15756914e-02 -1.47262895e-02 -2.12996639e-02 -1.02374870e-02\n",
      "  4.49680910e-03  3.05707045e-02  3.76468664e-03  4.18131938e-03\n",
      "  3.09329592e-02 -1.16485450e-02 -6.57146703e-03 -9.60067008e-03\n",
      "  2.40553133e-02 -1.46127201e-03  1.20606031e-02 -1.07682385e-02\n",
      "  1.72492974e-02  4.06426452e-02  1.16134882e-02  2.39669792e-02\n",
      "  7.75962835e-03 -2.80176941e-02  2.15718383e-03 -6.77349279e-03\n",
      "  5.81156043e-03 -8.82896036e-03 -1.43229757e-02 -8.32262635e-03\n",
      "  1.57379247e-02 -3.18030943e-03  2.47061085e-02 -1.46161474e-03\n",
      " -1.87225882e-02  2.96704769e-02  5.85202314e-02  1.84921585e-02\n",
      "  5.00791967e-02 -1.98999923e-02  1.54566942e-02  5.77560219e-04\n",
      "  1.23061389e-02  1.78496018e-02 -2.13585757e-02 -2.54523195e-02\n",
      " -1.58948619e-02 -7.31809158e-03 -1.37340752e-02  1.88370720e-02\n",
      "  3.64981999e-04  3.25183235e-02  2.06097439e-02 -1.84067022e-02\n",
      "  4.39370796e-02 -8.70193727e-03  1.36057539e-02  7.34321168e-03\n",
      "  2.93072909e-02 -1.95480809e-02 -1.67624303e-03  3.00019421e-02\n",
      " -4.34398232e-03 -1.94175052e-03  2.31424486e-03  2.58363970e-03\n",
      "  1.27937761e-03 -2.54854281e-02 -6.02927234e-04  6.63413554e-02\n",
      "  5.61503693e-04  1.19309407e-02  1.46496957e-02  1.89670976e-02\n",
      "  4.36349260e-03  6.27123867e-04  1.94862913e-02  1.71267819e-02\n",
      "  2.19595954e-02  2.50589307e-02  5.34947664e-02  2.39473712e-02\n",
      " -2.29764581e-02  1.14443274e-02  1.09391771e-02  4.18188376e-03\n",
      "  4.21404317e-02  2.55005509e-02 -3.67133729e-02  1.91569831e-02\n",
      "  3.84159721e-02 -4.82820021e-03 -3.34706833e-03 -8.04805950e-06\n",
      "  1.26599064e-02  6.01522205e-03 -1.36437649e-02 -8.11251067e-03\n",
      "  2.82883141e-02  2.05243733e-02  5.00744442e-03 -2.16937847e-02\n",
      " -1.56498495e-02  1.81802306e-02  1.21447379e-02  8.57586041e-03\n",
      "  1.97315253e-02  2.92563196e-02 -1.30562503e-02  3.24233808e-02\n",
      "  3.49158272e-02 -5.73315844e-03 -3.08501180e-02  4.85831499e-02\n",
      " -1.66016091e-02 -3.14817578e-02  5.63362287e-03  2.21349001e-02\n",
      " -1.72647890e-02 -6.25088951e-03  1.10647790e-02  3.33088003e-02\n",
      "  4.64524813e-02  9.15630069e-03  3.47013809e-02  1.34377126e-02\n",
      " -1.95927843e-02 -2.95759998e-02  4.18265872e-02  2.51473188e-02\n",
      " -2.58920938e-02  3.22805159e-02  3.98238227e-02  2.17182152e-02\n",
      "  1.43034300e-02  2.45315507e-02  1.37039684e-02  2.80251639e-04\n",
      " -2.75784563e-02  3.48987058e-02 -1.67489089e-02  3.35066728e-02\n",
      " -4.38186573e-03  5.69165386e-02 -2.09839083e-02 -3.44291888e-02\n",
      "  3.12874168e-02  3.56086232e-02  6.32288158e-02 -1.30041149e-02\n",
      "  5.05495854e-02  3.55227245e-03 -1.00088096e-03  5.09921387e-02\n",
      " -2.38237213e-02 -9.63489991e-03  3.74320038e-02 -2.71522012e-02\n",
      "  3.51511203e-02 -1.75599679e-02 -2.70362254e-02 -2.93632913e-02\n",
      " -7.36497622e-03  3.88371386e-02 -2.24388689e-02 -9.52695403e-03\n",
      "  1.43561037e-02 -2.34147115e-03 -9.08689387e-03  1.62720364e-02\n",
      "  3.56057994e-02 -2.83161364e-02 -3.99000980e-02  5.45975231e-02\n",
      "  2.68998146e-02  4.50867936e-02  4.41261977e-02  4.20004055e-02\n",
      "  2.65697613e-02 -2.69388352e-02  2.63450611e-02 -1.46101164e-02\n",
      "  4.73998524e-02 -7.16304220e-03  1.58548132e-02 -2.17281356e-02\n",
      "  3.97812948e-03  2.72459183e-02  4.77474295e-02 -1.94525383e-02\n",
      " -9.76198353e-04  1.04134232e-02 -2.09449735e-02  2.02104095e-02\n",
      "  3.30519639e-02  3.17354873e-02 -2.94794105e-02  2.18099747e-02\n",
      " -2.85388138e-02  3.33865583e-02 -2.06166487e-02  3.68251428e-02\n",
      "  3.47215831e-02  1.04129333e-02  4.26486358e-02 -2.00242847e-02\n",
      "  2.48729736e-02 -2.60779895e-02 -4.37400639e-02 -2.15101950e-02\n",
      " -4.60234750e-03 -2.13356335e-02  3.00826970e-02  1.29412096e-02\n",
      "  6.19026786e-03 -1.79500710e-02 -2.60382518e-02 -3.92091200e-02\n",
      " -1.79843772e-02  7.24796919e-05 -3.03851943e-02  4.58100848e-02\n",
      "  9.93830897e-03  5.19457385e-02 -1.19550787e-02  3.97700481e-02\n",
      "  1.97045784e-02 -2.25761179e-02 -2.89055128e-02  1.00255094e-03\n",
      " -3.28296684e-02  5.08736726e-03  4.14577499e-03 -2.31630653e-02\n",
      "  4.18650173e-02 -5.50882798e-03 -2.29301732e-02  3.08641400e-02\n",
      "  9.79745947e-03 -2.29308102e-02  1.48663437e-02 -1.60902124e-02\n",
      "  1.87583137e-02  1.90300513e-02  9.26150475e-03 -1.94002278e-02\n",
      " -4.28622216e-03  4.32906970e-02  4.76418398e-02  4.10948209e-02\n",
      "  1.54853482e-02  3.14983577e-02 -7.16090761e-03  1.72331724e-02\n",
      "  3.20425592e-02  1.87568832e-02 -3.27011906e-02 -1.83154549e-02\n",
      "  3.03755738e-02  1.26010990e-02  2.77973525e-03  5.90650178e-03\n",
      "  5.68308458e-02  2.44869012e-02  4.37581614e-02  8.39379616e-03\n",
      " -3.06898579e-02 -3.06645148e-02  8.10499489e-03  1.18337087e-02\n",
      " -1.46200573e-02 -2.77435258e-02  4.54149991e-02  8.61545093e-04\n",
      "  1.69488240e-03  5.07170986e-03  6.95732480e-04 -2.69115530e-02\n",
      "  5.51067144e-02  1.71042606e-02  1.62161160e-02 -3.94731387e-02\n",
      "  2.33494341e-02  1.64121017e-02 -2.02129837e-02  1.57977119e-02]\n",
      "[[ 5.2992824e-02  3.2164533e-02 -1.6898556e-04 ...  3.6567652e-03\n",
      "   8.4848953e-03  4.2981271e-02]\n",
      " [ 7.0832531e-05 -3.2623276e-02  4.2245854e-02 ...  2.8206028e-02\n",
      "  -2.8752230e-02  1.0796635e-02]\n",
      " [ 4.7532633e-02 -2.7713820e-03 -3.7825946e-02 ...  3.0386787e-02\n",
      "  -7.3034028e-03 -4.4971946e-04]\n",
      " ...\n",
      " [ 2.8597157e-02 -2.7177397e-02  1.7866444e-02 ... -5.7467665e-03\n",
      "   2.0201674e-02  4.3983579e-02]\n",
      " [-1.0836293e-02  3.2107450e-02  3.9409991e-02 ... -2.3052908e-02\n",
      "   1.9479988e-02  2.7092598e-02]\n",
      " [ 4.1464645e-02 -1.2843261e-03 -9.9996142e-03 ...  3.8894575e-02\n",
      "  -2.7988404e-02  2.9607695e-02]]\n",
      "[-0.03980738  0.03572765 -0.04497303  0.06721569 -0.03935601  0.00426069\n",
      "  0.0226273   0.03157442  0.02936028  0.00484555  0.0247135  -0.04938907\n",
      " -0.0365496   0.05440177  0.02500215  0.0748921   0.01154808  0.04825052\n",
      "  0.03876564 -0.05259171  0.03839367  0.058493    0.02501615  0.02387452\n",
      " -0.02513984  0.04381354 -0.01841255  0.02833161  0.01058124 -0.04751133\n",
      "  0.00225418  0.05136589  0.0422712   0.04144076 -0.01279331  0.02921551\n",
      "  0.01331545  0.0365443   0.01504234 -0.02962722  0.01308136  0.01636019\n",
      "  0.04679174 -0.01251374  0.04244975  0.02263984  0.02798804  0.04091493\n",
      "  0.02220247 -0.05082829  0.03151751 -0.00458084  0.04937458  0.01481925\n",
      " -0.03747951  0.01425065  0.01608253  0.04651424  0.00723798 -0.00918876\n",
      "  0.04788414  0.00899794  0.03126994  0.0134527   0.04619938 -0.02242889\n",
      " -0.01864842 -0.04771913  0.03975802  0.06545214  0.06920534 -0.0135776\n",
      "  0.0314203  -0.01280874  0.01291784 -0.02896015  0.01336689  0.02153889\n",
      "  0.03303704 -0.02045218 -0.0070068  -0.01969248  0.03507971 -0.03029658\n",
      "  0.0107324   0.01817605  0.05734377  0.01954434  0.0501101  -0.03204009\n",
      "  0.02969144  0.02857239  0.02788066 -0.03598033  0.02551888  0.01228011\n",
      " -0.02757299  0.02539387 -0.01732377  0.00183347  0.00884376 -0.03921192\n",
      "  0.01875606  0.0244035  -0.00069516  0.05070908 -0.01871817  0.04858595\n",
      " -0.00839615  0.04354167  0.00796487 -0.04267598  0.02408312  0.05892953\n",
      "  0.00449438 -0.03406593 -0.02837205  0.00885378  0.0122598   0.02267568\n",
      "  0.0004313  -0.04087647 -0.01727961 -0.03072804  0.05114163 -0.02704106\n",
      "  0.05642145  0.00846598  0.0494752  -0.05630522 -0.0036918  -0.01601929\n",
      " -0.03452645 -0.03887863  0.03461016 -0.05275114 -0.01495515 -0.0399991\n",
      "  0.01638109 -0.02291923  0.06006243 -0.03498901 -0.02882873 -0.00654792\n",
      "  0.03106936 -0.02766716  0.02048421 -0.04024355  0.03034917  0.00061454\n",
      " -0.00088046  0.00421452  0.05946131  0.02446955  0.01758522  0.02774477\n",
      " -0.02944066 -0.00338912 -0.02982351  0.02907417 -0.02946286  0.04817568\n",
      "  0.04085739 -0.0044245  -0.01935611 -0.03776058  0.0510394   0.0333044\n",
      " -0.00888608  0.00067799 -0.04330134  0.04081621  0.02692848  0.0131759\n",
      " -0.02270498 -0.02679629 -0.02124774 -0.01699183  0.00011046 -0.00785634\n",
      " -0.04000388  0.00026022  0.02302881 -0.03711759  0.04710233 -0.03259198\n",
      "  0.02912694  0.03328063 -0.01586726  0.06867091  0.01747843  0.0308918\n",
      "  0.03863955 -0.0359499   0.00402833 -0.05549552  0.00288459 -0.03368909\n",
      "  0.09075383 -0.00771287  0.01160081  0.01246216 -0.00157421  0.02796772\n",
      " -0.01177196 -0.02726274  0.01329505  0.02122916 -0.00892177 -0.02894719\n",
      " -0.00261113  0.00851478  0.02985901  0.03426195  0.06434793 -0.02794122\n",
      " -0.00578252 -0.00595274 -0.01644128  0.04000217  0.0329486  -0.0274076\n",
      "  0.01075008  0.03689646  0.0189315  -0.04130712 -0.03282614 -0.04372872\n",
      " -0.02631062  0.0300002  -0.00777492 -0.04297972 -0.03327313  0.01524864\n",
      " -0.01295416  0.04232956  0.01817959  0.05098483 -0.00599128 -0.00199549\n",
      " -0.00452628 -0.02490119  0.07720446  0.05927856  0.0046762  -0.00385054\n",
      " -0.01658504  0.01139155  0.03646757 -0.00362264 -0.03247304 -0.01260944\n",
      "  0.00102588  0.03006053 -0.00832942  0.00130846  0.04448299 -0.03039345\n",
      " -0.01239191  0.06047019 -0.01076129  0.05634224  0.0072038   0.00448697\n",
      "  0.04948564 -0.03309931  0.01459816 -0.01908666  0.0591467  -0.04054645\n",
      "  0.01997306 -0.0453523  -0.03789791  0.01536834  0.02448208  0.04003768\n",
      " -0.00776151 -0.02729406 -0.03834675 -0.01511348  0.01721752  0.03160969\n",
      " -0.00800136 -0.00742689  0.00084146 -0.00369778  0.00128213  0.05278472\n",
      " -0.03401988  0.02202801 -0.01612976 -0.04841764 -0.03122615 -0.01884689\n",
      "  0.00871182  0.05959836  0.01940713  0.0356066  -0.01888047 -0.02250032\n",
      "  0.02216798 -0.03756418  0.02165994  0.03627621  0.03618426 -0.00299731\n",
      "  0.01855629 -0.03865527 -0.01279181 -0.03872218  0.06357958  0.00225704\n",
      " -0.01083533  0.03100619 -0.01652316 -0.01974258  0.05052001 -0.00108911\n",
      " -0.02890005  0.00751739 -0.01502027 -0.00236709  0.04458006 -0.07066642\n",
      "  0.06015036 -0.0050031   0.04904402  0.01520682 -0.01870155 -0.01793758\n",
      "  0.06177054 -0.01537685  0.00523363 -0.04087367 -0.02026419  0.04779961\n",
      "  0.09110975  0.05746897  0.00330217  0.02959141  0.0074905  -0.03198398\n",
      " -0.00338408  0.05385716  0.01254587  0.00898174 -0.04044325  0.00280561\n",
      "  0.04580487 -0.00225513  0.05958559 -0.0054231  -0.00046884 -0.03287969\n",
      " -0.04136391  0.03546718 -0.03754776  0.02767434  0.06078355  0.02131928\n",
      "  0.02583835 -0.01008096  0.04577499  0.00177018  0.04111928 -0.03263802\n",
      " -0.00769691  0.04114348  0.05117305 -0.03575861 -0.04059547 -0.00402968\n",
      " -0.00443333  0.02587904 -0.05537295  0.02586835  0.00645031  0.04066646\n",
      "  0.01689376  0.03758162  0.01996851 -0.02746943 -0.01062248 -0.01112357\n",
      " -0.0149505   0.06689324  0.02740761 -0.01982518 -0.00328896 -0.01039248\n",
      "  0.01514381  0.02195494  0.05269011  0.02307127  0.02905982  0.03486244\n",
      "  0.00810434 -0.0036364   0.01696952  0.04345626 -0.00988002  0.06252107\n",
      "  0.0073309   0.06359293 -0.0027946  -0.0283721  -0.01282117  0.0109444\n",
      " -0.03105059  0.02862681 -0.01710439  0.00545503 -0.01205118 -0.03303625\n",
      "  0.03536351  0.02549541 -0.0136213   0.05593491 -0.04110643  0.03868954\n",
      " -0.02020075 -0.02029233  0.03340875 -0.03690168  0.03958283 -0.03589233\n",
      "  0.08779428 -0.01551084 -0.02295537 -0.00717428  0.01953859 -0.00773712\n",
      "  0.04015234  0.0366557  -0.02386483 -0.0205278  -0.04095446  0.00632526\n",
      " -0.01109104  0.00039419 -0.02183168 -0.01697147 -0.03303045 -0.00258405\n",
      " -0.00763311  0.03684556  0.03343259 -0.04128624  0.05000857  0.01700387\n",
      " -0.000109    0.02677676 -0.02653043  0.02470022  0.05713122 -0.02861839\n",
      "  0.00811194  0.0196728  -0.01848997 -0.00226211 -0.00906148  0.06814981\n",
      " -0.03699832 -0.02661506  0.04473102 -0.02692185  0.05776942  0.02427801\n",
      " -0.01437159 -0.00691733 -0.02764469  0.01616487  0.08472864  0.03389421\n",
      " -0.01400001  0.03254932 -0.00327239  0.03013456  0.0343967   0.03568677\n",
      " -0.02573878  0.02782855  0.0134298  -0.02910917  0.00458905 -0.00947115\n",
      "  0.02517797  0.04188962  0.02650472 -0.02385665 -0.00409169 -0.02671499\n",
      " -0.0207858  -0.00260724 -0.00985791  0.03477061 -0.01806321 -0.02899208\n",
      " -0.00838954  0.06147132 -0.04584699 -0.00571893  0.02279609 -0.00476819\n",
      " -0.03490019 -0.04261985  0.01045747  0.06171109  0.0703631   0.02549672\n",
      "  0.05416097  0.03394729]\n",
      "[[ 0.06987834 -0.08474834 -0.00728399 ...  0.0009581  -0.07348651\n",
      "   0.02516182]\n",
      " [ 0.00044961 -0.00911046  0.04661714 ...  0.01631049  0.0539348\n",
      "  -0.03512327]\n",
      " [ 0.09063087  0.01036997 -0.0558193  ...  0.04303304 -0.00605464\n",
      "  -0.10630448]\n",
      " ...\n",
      " [ 0.06201733  0.02300742 -0.02449629 ... -0.02927207 -0.06126519\n",
      "  -0.02952384]\n",
      " [ 0.04842322 -0.03300626  0.0216815  ... -0.04839092 -0.10212715\n",
      "  -0.00680856]\n",
      " [-0.04377083 -0.05572006  0.01087095 ... -0.06149076 -0.05948809\n",
      "   0.13821419]]\n",
      "[-0.10692628  0.21979462 -0.01194234 -0.06001273  0.02708632  0.18249236\n",
      " -0.0373331   0.089274   -0.19904873 -0.02901145]\n"
     ]
    }
   ],
   "source": [
    "for key, value in model.state_dict().items():\n",
    "  print(value.cpu().detach().numpy())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
